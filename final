import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import numpy as np
import os
from tensorflow.keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
# Set up some parameters
img_width, img_height = 128, 128  # We’re using 128x128 images for this project
num_classes = 4  # We have 4 different classes (gotta have variety)
batch_size = 32  # Batch size of 32 to keep things moving at a steady pace
num_train_samples = 1000  # 1000 images in the training set (not bad)
num_test_samples = 200  # 200 images for testing, so we can see if the model’s actually working
epochs = 10  # We’re gonna train for 10 epochs, gotta see if the model improves
# Create synthetic data (random images and mock labels)
def generate_synthetic_data(num_samples, img_width, img_height, num_classes):
    X = np.random.rand(num_samples, img_width, img_height, 3)  # Create random RGB images (nothing fancy yet)
    y = np.random.randint(0, num_classes, num_samples)  # Random labels for our 4 classes (just to make it work for now)
    y = to_categorical(y, num_classes)  # One-hot encoding of labels, turns them into a usable format
    return X, y
# Simulate training and test data (no real dataset here, just simulating)
X_train, y_train = generate_synthetic_data(num_train_samples, img_width, img_height, num_classes)
X_test, y_test = generate_synthetic_data(num_test_samples, img_width, img_height, num_classes)
# Create ImageDataGenerator for data preprocessing
train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale images to [0,1] for the model
test_datagen = ImageDataGenerator(rescale=1./255)  # Same for test images, no exceptions
# Simulate flow_from_directory behavior with numpy arrays (since we don’t have directories yet)
train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)  # Feeding in the training data
test_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)  # Feeding in the test data
# Build the model
model = Sequential([  
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),  # First Conv layer to learn basic features
    MaxPooling2D(2, 2),  # Pooling layer to reduce image size and focus on important features
    Conv2D(64, (3, 3), activation='relu'),  # Second Conv layer to learn more complex patterns
    MaxPooling2D(2, 2),  # Pooling layer to shrink image data even more
    Conv2D(128, (3, 3), activation='relu'),  # Third Conv layer, really diving deep now
    MaxPooling2D(2, 2),  # Pooling, keeping the important stuff and throwing away the extra
    Flatten(),  # Flatten the 3D data to 1D so it can be processed by the fully connected layers
    Dense(256, activation='relu'),  # Fully connected layer with 256 neurons (more power!)
    Dropout(0.5),  # Dropout to avoid overfitting (basically letting the model learn without memorizing)
    Dense(num_classes, activation='softmax')  # Output layer, softmax to predict the class probabilities
])
# Compile the model (set up the optimizer, loss function, and performance metric)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Adam optimizer is solid for most tasks
# Training the model
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)  # Stop training early if it's not improving
history = model.fit(
    train_generator,  # Training with the training data
    epochs=epochs,  # Run for 10 epochs
    validation_data=test_generator,  # Validate with the test data after each epoch
    callbacks=[early_stopping]  # Early stopping if validation loss doesn’t get better
)
# Evaluate the model on the test data
loss, accuracy = model.evaluate(test_generator)  # Get the final accuracy and loss on the test data
print(f'Test Accuracy: {accuracy * 100:.2f}%')  # Print the test accuracy to see how well it did
# Plot accuracy and loss graphs to visualize model performance
plt.figure(figsize=(12, 4))  # Set up a big enough plot to show both graphs side by side
# Plot accuracy graph
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')  # Plot training accuracy
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Plot validation accuracy
plt.title('Accuracy over Epochs')  # Show how accuracy changes over the epochs
plt.xlabel('Epoch')  # Label the x-axis with "Epoch"
plt.ylabel('Accuracy')  # Label the y-axis with "Accuracy"
plt.legend()  # Show the legend to distinguish between training and validation
# Plot loss graph
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')  # Plot training loss
plt.plot(history.history['val_loss'], label='Validation Loss')  # Plot validation loss
plt.title('Loss over Epochs')  # Show how loss changes over the epochs
plt.xlabel('Epoch')  # Label the x-axis with "Epoch"
plt.ylabel('Loss')  # Label the y-axis with "Loss"
plt.legend()  # Show the legend to distinguish between training and validation
# Display the plots
plt.show()
